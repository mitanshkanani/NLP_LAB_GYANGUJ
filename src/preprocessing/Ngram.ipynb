{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064d8b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up NLTK...\n",
      "✓ punkt tokenizer already installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Setting up NLTK...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"✓ punkt tokenizer already installed\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK's 'punkt' model...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"✓ punkt tokenizer installed\")\n",
    "\n",
    "def load_tokens_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .txt files from a folder and returns a single list of all words.\n",
    "    Optimized for Gujarati text.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    file_paths = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"Error: No .txt files found in the folder '{folder_path}'.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Found {len(file_paths)} files to analyze:\")\n",
    "    for file_path in file_paths:\n",
    "        print(f\"  - {os.path.basename(file_path)}\")\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Simple split works best for Gujarati\n",
    "                tokens = line.strip().split()\n",
    "                all_tokens.extend(tokens)\n",
    "                \n",
    "    return all_tokens\n",
    "\n",
    "def analyze_unigrams(tokens, top_n=50):\n",
    "    \"\"\"\n",
    "    Analyzes unigram frequencies and probabilities.\n",
    "    \"\"\"\n",
    "    report = [\"=\"*20 + \" UNIGRAM ANALYSIS \" + \"=\"*20]\n",
    "    report.append(f\"Total words (tokens): {len(tokens)}\")\n",
    "    report.append(f\"Unique words (vocabulary): {len(set(tokens))}\\n\")\n",
    "    \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    \n",
    "    report.append(f\"--- Top {top_n} Most Common Words ---\\n\")\n",
    "    report.append(f\"{'Rank':<5} {'Word':<20} {'Count':<10} {'Probability (%)':<15}\")\n",
    "    report.append(\"-\" * 60)\n",
    "    \n",
    "    for i, (word, count) in enumerate(fdist.most_common(top_n), 1):\n",
    "        probability = (count / len(tokens)) * 100\n",
    "        report.append(f\"{i:<5} {word:<20} {count:<10} {probability:.4f}%\")\n",
    "        \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def analyze_bigrams(tokens, top_n=50):\n",
    "    \"\"\"\n",
    "    Analyzes bigram frequencies and conditional probabilities.\n",
    "    \"\"\"\n",
    "    report = [\"\\n\" + \"=\"*20 + \" BIGRAM ANALYSIS \" + \"=\"*20]\n",
    "    bigram_list = list(ngrams(tokens, 2))\n",
    "    report.append(f\"Total bigrams: {len(bigram_list)}\\n\")\n",
    "    \n",
    "    fdist = nltk.FreqDist(bigram_list)\n",
    "    \n",
    "    report.append(f\"--- Top {top_n} Most Common Word Pairs ---\\n\")\n",
    "    report.append(f\"{'Rank':<5} {'Bigram':<30} {'Count':<10} {'Probability (%)':<15}\")\n",
    "    report.append(\"-\" * 70)\n",
    "    \n",
    "    for i, (bigram, count) in enumerate(fdist.most_common(top_n), 1):\n",
    "        probability = (count / len(bigram_list)) * 100\n",
    "        bigram_str = ' '.join(bigram)\n",
    "        report.append(f\"{i:<5} {bigram_str:<30} {count:<10} {probability:.4f}%\")\n",
    "        \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "def analyze_trigrams(tokens, top_n=50):\n",
    "    \"\"\"\n",
    "    Analyzes trigram frequencies and conditional probabilities.\n",
    "    \"\"\"\n",
    "    report = [\"\\n\" + \"=\"*20 + \" TRIGRAM ANALYSIS \" + \"=\"*20]\n",
    "    trigram_list = list(ngrams(tokens, 3))\n",
    "    report.append(f\"Total trigrams: {len(trigram_list)}\\n\")\n",
    "    \n",
    "    fdist = nltk.FreqDist(trigram_list)\n",
    "    \n",
    "    report.append(f\"--- Top {top_n} Most Common Word Sequences ---\\n\")\n",
    "    report.append(f\"{'Rank':<5} {'Trigram':<40} {'Count':<10} {'Probability (%)':<15}\")\n",
    "    report.append(\"-\" * 80)\n",
    "    \n",
    "    for i, (trigram, count) in enumerate(fdist.most_common(top_n), 1):\n",
    "        probability = (count / len(trigram_list)) * 100\n",
    "        trigram_str = ' '.join(trigram)\n",
    "        report.append(f\"{i:<5} {trigram_str:<40} {count:<10} {probability:.4f}%\")\n",
    "        \n",
    "    return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5380a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting N-gram Analysis...\n",
      "Found 8 files to analyze:\n",
      "  - class11_biology.txt\n",
      "  - class11_chemistry.txt\n",
      "  - class11_maths.txt\n",
      "  - class11_physics.txt\n",
      "  - class12_biology.txt\n",
      "  - class12_chemistry.txt\n",
      "  - class12_maths.txt\n",
      "  - class12_physics.txt\n",
      "Processing: class11_biology.txt\n",
      "Processing: class11_chemistry.txt\n",
      "Processing: class11_maths.txt\n",
      "Processing: class11_physics.txt\n",
      "Processing: class12_biology.txt\n",
      "Processing: class12_chemistry.txt\n",
      "Processing: class12_maths.txt\n",
      "Processing: class12_physics.txt\n",
      "Successfully loaded 867503 tokens.\n",
      "Analyzing unigrams...\n",
      "Analyzing bigrams...\n",
      "Analyzing trigrams...\n",
      "\n",
      "✅ Analysis complete. Report saved to '../../data/ngram_analysis_report_ocr.txt'.\n",
      "\n",
      "--- Quick Preview ---\n",
      "Total tokens: 867503\n",
      "Unique words: 119901\n",
      "\n",
      "Top 5 most common words:\n",
      "  'છે.': 25673 times\n",
      "  'અને': 18401 times\n",
      "  '-': 12092 times\n",
      "  'છે': 10840 times\n",
      "  'કે': 8001 times\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "print(\"Starting N-gram Analysis...\")\n",
    "\n",
    "# Use your PREPROCESSED data folder\n",
    "input_folder = '../../data/processed'\n",
    "output_report_file = '../../data/ngram_analysis_report_ocr.txt'\n",
    "\n",
    "# 1. Load all tokens from the preprocessed files\n",
    "all_tokens = load_tokens_from_folder(input_folder)\n",
    "\n",
    "if all_tokens:\n",
    "    print(f\"Successfully loaded {len(all_tokens)} tokens.\")\n",
    "    \n",
    "    # 2. Perform analysis for each N-gram type\n",
    "    print(\"Analyzing unigrams...\")\n",
    "    unigram_report = analyze_unigrams(all_tokens)\n",
    "    \n",
    "    print(\"Analyzing bigrams...\")\n",
    "    bigram_report = analyze_bigrams(all_tokens)\n",
    "    \n",
    "    print(\"Analyzing trigrams...\")\n",
    "    trigram_report = analyze_trigrams(all_tokens)\n",
    "    \n",
    "    # 3. Combine reports and save to a file\n",
    "    import datetime\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    final_report = f\"\"\"\n",
    "N-GRAM ANALYSIS REPORT\n",
    "=======================\n",
    "Dataset Folder: {os.path.abspath(input_folder)}\n",
    "Analysis Date: {current_time}\n",
    "\n",
    "{unigram_report}\n",
    "\n",
    "{bigram_report}\n",
    "\n",
    "{trigram_report}\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_report)\n",
    "        \n",
    "    print(f\"\\n✅ Analysis complete. Report saved to '{output_report_file}'.\")\n",
    "    \n",
    "    # Show quick preview\n",
    "    print(\"\\n--- Quick Preview ---\")\n",
    "    print(f\"Total tokens: {len(all_tokens)}\")\n",
    "    print(f\"Unique words: {len(set(all_tokens))}\")\n",
    "    \n",
    "    # Show top 5 most common words\n",
    "    word_freq = Counter(all_tokens)\n",
    "    print(\"\\nTop 5 most common words:\")\n",
    "    for word, count in word_freq.most_common(5):\n",
    "        print(f\"  '{word}': {count} times\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Failed to load tokens. Please check the folder path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
